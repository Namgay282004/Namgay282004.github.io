---
Title: DAM101 UnitFour
categories: [DAM101, UnitFour]
tags: [DAM101]
---
## Topic : Hyperparameter Tuning, Regularisation and Optimization

When building a deep learning model there are some factors or parameters that decide the efficiency of our deep learning model. For a Neural Network these parameters correspond to the ‘W’: Weight and ‘b’: Bias

Along with these parameters there are some parameters which control how our algorithm works these parameters are known as the hyper parameters namely:

- Learning rate.

- Number of iterations.

- Number of hidden layers L.

- Number of hidden units n.

- Choice of activation functions.

It’s difficult to get all the hyper parameters right on a new application at the first time. It usually involves a trial-error approach followed by most of the researchers as these parameters also vary for different domains of problems i.e. (NLP, Computer Vision etc.) there is no fixed set of values to be used and implemented for a given problem statement. So, the idea is you go through the loop:

![alt text](<../Images_for DAM101/Screenshot from 2024-05-19 14-12-05.png>)

We have to go through the loop many times to figure out the hyper parameters. This will help figure out the right set of values for a given problem statement. For that it is important to understand how to train our model to get a result or to solve the problem using Deep Learning /Machine Learning Algorithms.

### Train/Test/Dev sets

The data is split into three parts:

1. **Training set** (Usually the largest part of dataset)

2. **Validation / Development set**

3. **Testing set**

The idea is to build a model upon training set, then try to optimize hyper parameters on Dev set as much as possible. Then after your model is ready you try and evaluate the testing set.

### Bias/Variance

To understand and properly evaluate the performance of Machine Learning models, in particular the Supervised Machine Learning models it is important to understand the bias-variance trade off.

- If your model is under-fitting (logistic regression of nonlinear data) it has “high bias”.

- If your model is over-fitting, then it has a “high variance”.

- Your model will be alright if you balance the Bias / Variance.

This picture is from the Lectures by Andrew Ng in the course showing the different cases of variances diagrammatically

![alt text](<../Images_for DAM101/Screenshot from 2024-05-19 14-40-08.png>)

### Best Recipe for Machine Learning

**If the algorithm has a high bias**:

- Try making your NN bigger (size of hidden units, number of layers)

- Try a different model that is suitable for your data.

- Try to run it longer.

- Different (advanced) optimization algorithms.

**If the algorithm has a high variance**:

- Train more data.

- Try regularization.

- Try a different model that is suitable for your data.

- Try the previous two points until you have a low bias and low variance.